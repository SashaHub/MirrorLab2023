{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1joZc_V8Zzk_YDw1Q_my99LTsNMCfFdf0",
      "authorship_tag": "ABX9TyMjkBKEIzfN0//F1+cIVpMb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫"
      ],
      "metadata": {
        "id": "v7O4MBIQvwCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫\n",
        "!pip install gensim transformers torch scikit-learn nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VR-Oab7EvvLm",
        "outputId": "f6ad2499-fe1b-4485-f981-b1a531298776"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –ò–º–ø–æ—Ä—Ç"
      ],
      "metadata": {
        "id": "2zQL4U34v1NH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0X3XqCv_v48D",
        "outputId": "1fe46138-dacd-4068-a7de-162fc63b112a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten_to_text(value):\n",
        "    \"\"\"\n",
        "    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –≤ —Ç–µ–∫—Å—Ç:\n",
        "    - –ú–∞—Å—Å–∏–≤—ã: –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è –≤ —Å—Ç—Ä–æ–∫—É —á–µ—Ä–µ–∑ –ø—Ä–æ–±–µ–ª.\n",
        "    - –°–ª–æ–≤–∞—Ä–∏: –∑–Ω–∞—á–µ–Ω–∏—è –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è –≤ —Å—Ç—Ä–æ–∫—É —á–µ—Ä–µ–∑ –ø—Ä–æ–±–µ–ª.\n",
        "    - –î—Ä—É–≥–∏–µ —Ç–∏–ø—ã: –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É—é—Ç—Å—è –≤ —Å—Ç—Ä–æ–∫—É –Ω–∞–ø—Ä—è–º—É—é.\n",
        "    - –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∑–∞–º–µ–Ω—è—é—Ç—Å—è –Ω–∞ –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if isinstance(value, list):  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è —Å–ø–∏—Å–∫–æ–≤\n",
        "            return \" \".join(map(str, value))\n",
        "        elif isinstance(value, dict):  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è —Å–ª–æ–≤–∞—Ä–µ–π\n",
        "            return \" \".join(map(str, value.values()))\n",
        "        elif pd.isnull(value):  # –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
        "            return \"\"\n",
        "        else:\n",
        "            return str(value)  # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Ç–∏–ø–æ–≤\n",
        "    except Exception as e:\n",
        "        print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–Ω–∞—á–µ–Ω–∏—è: {value}, –æ—à–∏–±–∫–∞: {e}\")\n",
        "        return \"\"  # –í–æ–∑–≤—Ä–∞—Ç –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–∏ –ø—Ä–∏ –æ—à–∏–±–∫–µ\n",
        "\n",
        "def preprocess_dataframe(data, text_columns):\n",
        "    \"\"\"\n",
        "    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —É–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ:\n",
        "    - –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –º–∞—Å—Å–∏–≤—ã, —Å–ª–æ–≤–∞—Ä–∏ –∏ –¥—Ä—É–≥–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Å—Ç—Ä–æ–∫–∏.\n",
        "    - –û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø–æ–ª—è –≤ –æ–¥–∏–Ω —Å—Ç–æ–ª–±–µ—Ü.\n",
        "    \"\"\"\n",
        "    for col in text_columns:\n",
        "        data[col] = data[col].apply(flatten_to_text)  # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º –æ–±—â–∏–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Å—Ç–æ–ª–±–µ—Ü\n",
        "    data['text'] = data[text_columns].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
        "    return data"
      ],
      "metadata": {
        "id": "TSZktiawxPeo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    –ü—Ä–æ—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞ —Ç–µ–∫—Å—Ç–∞:\n",
        "    - –ü—Ä–∏–≤–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É.\n",
        "    - –£–¥–∞–ª—è–µ—Ç –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é, —Å–ª—ç—à–∏, –∏ –ø—Ä–æ—á–∏–π \"–º—É—Å–æ—Ä\".\n",
        "    - –û—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã –∏ –ø—Ä–æ–±–µ–ª—ã.\n",
        "\n",
        "    Args:\n",
        "        text (str): –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç.\n",
        "\n",
        "    Returns:\n",
        "        str: –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç.\n",
        "    \"\"\"\n",
        "    # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
        "    text = text.lower()\n",
        "    # –£–¥–∞–ª–µ–Ω–∏–µ –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ (–æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã –∏ –ø—Ä–æ–±–µ–ª—ã)\n",
        "    text = re.sub(r'[^–∞-—è–ê-–Ø—ë–Åa-zA-Z\\s]', '', text)\n",
        "    # –ó–∞–º–µ–Ω–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –Ω–∞ –æ–¥–∏–Ω\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "GFrivz6207Ft"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
      ],
      "metadata": {
        "id": "tlSICXOWv4fZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/DocsNew1/epic_records_with_target.parquet'\n",
        "data = pd.read_parquet(file_path)\n",
        "use_columns = ['gender', 'amnez_life', 'amnez_disease', 'condition',\n",
        "       'ward_table', 'full_wards', 'complication_of_main_disease_desc',\n",
        "       'main_disease_desc', 'secondary_disease_desc']"
      ],
      "metadata": {
        "id": "krTuEXScwKbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = preprocess_dataframe(data, use_columns)"
      ],
      "metadata": {
        "id": "TX6r-oTLwdHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –∏ —Å—Ç–µ–º–º–∏–Ω–≥"
      ],
      "metadata": {
        "id": "bcZLvTtPx1I1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.data.path.append(\"/root/nltk_data\")"
      ],
      "metadata": {
        "id": "v7BcWpczzg2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('russian'))\n",
        "stemmer = SnowballStemmer(\"russian\")\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text, language=\"russian\")\n",
        "    tokens = [stemmer.stem(word) for word in tokens if word.isalpha() and word not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ (–¥–∞–Ω–Ω—ã–µ –¥–ª—è TF-IDF –∏ –º–æ–¥–µ–ª–∏ –±–µ–∑ embeddings –º–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π)\n",
        "data['processed_text'] = data['text'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "SKZ0OQuLx0cB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['processed_text_simple'] = data['text'].apply(clean_text)"
      ],
      "metadata": {
        "id": "l04iJyF20-QM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_parquet('/content/drive/MyDrive/DocsNew1/preprocessed_records.parquet')"
      ],
      "metadata": {
        "id": "btutjOXE3JrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. –û–±—É—á–µ–Ω–∏–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ word embedding –Ω–∞ –¥–∞–Ω–Ω—ã—Ö"
      ],
      "metadata": {
        "id": "8t0kS3S61NCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è word2vec\n",
        "tokenized_texts = [word_tokenize(text, language=\"russian\") for text in data['text']]"
      ],
      "metadata": {
        "id": "1p7o851S1IxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –æ–±—É—á–µ–Ω–∏–µ Word2Vec\n",
        "w2v_model = Word2Vec(\n",
        "    sentences=tokenized_texts,  # –¢–µ–∫—Å—Ç—ã –≤ –≤–∏–¥–µ —Ç–æ–∫–µ–Ω–æ–≤\n",
        "    vector_size=100,            # –†–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–∞ embeddings\n",
        "    window=5,                   # –û–∫–Ω–æ –¥–ª—è —Å–æ—Å–µ–¥–µ–π\n",
        "    min_count=10,               # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—Ö–æ–∂–¥–µ–Ω–∏–π —Å–ª–æ–≤–∞\n",
        "    workers=2,                  # –ß–∏—Å–ª–æ –ø–æ—Ç–æ–∫–æ–≤\n",
        "    sg=1                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Skip-gram\n",
        ")"
      ],
      "metadata": {
        "id": "vrze-HQ31Qn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
        "w2v_model_path = \"/content/drive/MyDrive/DocsNew1/word2vec_custom.model\"\n",
        "w2v_model.save(w2v_model_path)\n",
        "print(\"–û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å word embeddings —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ø–æ –ø—É—Ç–∏:\", w2v_model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-zxdMcj1Wn1",
        "outputId": "a3caa5b3-f1b8-4712-c477-ac572026b778"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å word embeddings —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ø–æ –ø—É—Ç–∏: /content/drive/MyDrive/DocsNew1/word2vec_custom.model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ embeddings\n",
        "print(\"–ü—Ä–∏–º–µ—Ä embedding –¥–ª—è —Å–ª–æ–≤–∞ '–∏–Ω—Ñ–∞—Ä–∫—Ç':\", w2v_model.wv['–∏–Ω—Ñ–∞—Ä–∫—Ç'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzYp9bcv1Yt7",
        "outputId": "99efbc9a-385f-4c04-f47b-a16bed1c052b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–ü—Ä–∏–º–µ—Ä embedding –¥–ª—è —Å–ª–æ–≤–∞ '–∏–Ω—Ñ–∞—Ä–∫—Ç': [ 0.29276365  0.6517586   0.5328845   0.73116666 -0.8496222  -0.636645\n",
            "  0.6585501   1.1890719  -0.79563886 -0.2130396  -0.42085025  0.15055671\n",
            "  0.07983494  0.31640658 -0.5890132   0.2721545   0.14484827 -0.5062335\n",
            " -0.23533274 -0.57325673  0.2820768   0.44494748  0.845124   -0.20620432\n",
            " -0.6613513  -0.32934397  0.24691     0.38782993 -0.50931334  0.29559615\n",
            "  0.84071743 -0.28958678 -0.27046475 -0.52372414  0.05379103  0.6286116\n",
            "  0.65153503 -0.80114216 -0.08808368 -1.1112344   1.2720308  -0.12672056\n",
            " -0.36405873  0.21855429  0.5761397   0.9261662  -0.20644742  0.27033344\n",
            " -0.30193034  0.8543009   0.504435   -0.8382857   0.00977375 -0.65603644\n",
            " -0.44226336 -0.00469937  0.7426868   0.22981463 -0.01551979  0.7230486\n",
            " -0.15901366 -1.0127788   0.9339757  -0.8905167  -0.7562497  -0.4298161\n",
            "  0.26092336  0.733011   -0.59222364  0.96409667  0.38321874 -0.4533126\n",
            "  1.1089891   0.48428282  0.13257563 -0.16951585  0.2496799  -0.5567076\n",
            " -0.01357115 -0.2820412   0.3497293   0.6562468  -0.55391735 -0.02178465\n",
            " -0.41447487 -0.5511931   0.21088259 -0.41327652 -0.2591817   0.19980976\n",
            " -0.29223067  0.62955976  0.49362403  0.44736508  0.24625517  0.61757976\n",
            "  0.68482053 -0.7540835   0.35114363 -0.8857855 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö word embeddings (–Ω–∞–ø—Ä–∏–º–µ—Ä, FastText –∏–ª–∏ Word2Vec)"
      ],
      "metadata": {
        "id": "C5eIBLSNzxW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# –£–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ word embeddings\n",
        "embedding_model_path = \"/content/drive/MyDrive/DocsNew1/word2vec_custom.model\"\n",
        "try:\n",
        "    word_vectors = KeyedVectors.load(embedding_model_path)\n",
        "    print(\"–ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ word embeddings —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç—å.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O039e2OkzB_c",
        "outputId": "ab7f716e-93a0-49c3-8c74-a7546152fa36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ word embeddings —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º TF-IDF"
      ],
      "metadata": {
        "id": "n2bhSfnG1teT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(max_features=500)\n",
        "tfidf_features = tfidf_vectorizer.fit_transform(data['processed_text']).toarray()\n",
        "\n",
        "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫–∏\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    data['processed_text'], data['target'], test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "COVWoS_I1wUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ word embeddings –∏ –º–æ–¥–µ–ª–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞"
      ],
      "metadata": {
        "id": "QLk875mH118C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_parquet('/content/drive/MyDrive/DocsNew1/preprocessed_records.parquet')"
      ],
      "metadata": {
        "id": "cigciFuHwN4l"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫–∏\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    data['processed_text_simple'], data['target'], test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "n2k68UVnv5Ct"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞\n",
        "model_name = 'alexyalunin/RuBioBERT'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)"
      ],
      "metadata": {
        "id": "WEI8RG_Q15AR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d724fc25-bafa-4bce-efe1-f77211af3b7e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at alexyalunin/RuBioBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.bert.parameters():\n",
        "    param.requires_grad = False\n",
        "#–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts.iloc[idx]\n",
        "        label = self.labels.iloc[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "GxlZIqbE186o"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\n",
        "train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
        "val_dataset = TextDataset(val_texts, val_labels, tokenizer)"
      ],
      "metadata": {
        "id": "5Dh4eWXl17Ty"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è"
      ],
      "metadata": {
        "id": "wDAUDMoL2DgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=50,\n",
        "    per_device_train_batch_size=64,\n",
        "    per_device_eval_batch_size=512,\n",
        "    learning_rate=1e-3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=13,\n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='eval_loss',\n",
        ")"
      ],
      "metadata": {
        "id": "XVvkwBZL2Ehk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62855175-f6e5-4d76-a2c8-c33aa4c18a99"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    roc_auc = roc_auc_score(labels, logits[:, 1])\n",
        "    return {'accuracy': acc, 'roc_auc': roc_auc}"
      ],
      "metadata": {
        "id": "RyT7vdKa2GUv"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞"
      ],
      "metadata": {
        "id": "AExoZqcI2Jji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "1NuQf_Xs2H63"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "0bIdvoKE2NcF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dd76382c-b211-48fc-d259-7243c751a023"
      },
      "execution_count": 31,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='521' max='650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [521/650 38:37 < 09:35, 0.22 it/s, Epoch 40/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Roc Auc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.678600</td>\n",
              "      <td>0.681775</td>\n",
              "      <td>0.547739</td>\n",
              "      <td>0.628571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.658700</td>\n",
              "      <td>0.668000</td>\n",
              "      <td>0.613065</td>\n",
              "      <td>0.624417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.644900</td>\n",
              "      <td>0.665685</td>\n",
              "      <td>0.613065</td>\n",
              "      <td>0.628166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.630800</td>\n",
              "      <td>0.663865</td>\n",
              "      <td>0.613065</td>\n",
              "      <td>0.631307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.624800</td>\n",
              "      <td>0.664629</td>\n",
              "      <td>0.613065</td>\n",
              "      <td>0.638804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.620600</td>\n",
              "      <td>0.662817</td>\n",
              "      <td>0.603015</td>\n",
              "      <td>0.644681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.622100</td>\n",
              "      <td>0.670263</td>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.647720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.620300</td>\n",
              "      <td>0.666841</td>\n",
              "      <td>0.608040</td>\n",
              "      <td>0.650051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.624700</td>\n",
              "      <td>0.662564</td>\n",
              "      <td>0.608040</td>\n",
              "      <td>0.650963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.613400</td>\n",
              "      <td>0.659566</td>\n",
              "      <td>0.618090</td>\n",
              "      <td>0.655117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.621200</td>\n",
              "      <td>0.664817</td>\n",
              "      <td>0.608040</td>\n",
              "      <td>0.659068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.616800</td>\n",
              "      <td>0.657235</td>\n",
              "      <td>0.618090</td>\n",
              "      <td>0.661601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.605900</td>\n",
              "      <td>0.656766</td>\n",
              "      <td>0.613065</td>\n",
              "      <td>0.664032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.613000</td>\n",
              "      <td>0.657825</td>\n",
              "      <td>0.618090</td>\n",
              "      <td>0.666363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.615400</td>\n",
              "      <td>0.654676</td>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.668997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.596900</td>\n",
              "      <td>0.653403</td>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.669504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.614800</td>\n",
              "      <td>0.655740</td>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.671935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.606100</td>\n",
              "      <td>0.651890</td>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.672543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.608000</td>\n",
              "      <td>0.650838</td>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.672948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.611500</td>\n",
              "      <td>0.653252</td>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.674063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.601000</td>\n",
              "      <td>0.651536</td>\n",
              "      <td>0.628141</td>\n",
              "      <td>0.674975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.609900</td>\n",
              "      <td>0.651449</td>\n",
              "      <td>0.628141</td>\n",
              "      <td>0.675684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.597200</td>\n",
              "      <td>0.648688</td>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.676089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.599500</td>\n",
              "      <td>0.648820</td>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.676292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.603600</td>\n",
              "      <td>0.649136</td>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.677001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.598300</td>\n",
              "      <td>0.648966</td>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.676190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.608700</td>\n",
              "      <td>0.648253</td>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.677710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.599300</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.678116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.602500</td>\n",
              "      <td>0.646846</td>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.680243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.595400</td>\n",
              "      <td>0.646123</td>\n",
              "      <td>0.633166</td>\n",
              "      <td>0.680142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.599600</td>\n",
              "      <td>0.647905</td>\n",
              "      <td>0.628141</td>\n",
              "      <td>0.680648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.601800</td>\n",
              "      <td>0.646105</td>\n",
              "      <td>0.633166</td>\n",
              "      <td>0.681054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.594000</td>\n",
              "      <td>0.645890</td>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.680750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.599200</td>\n",
              "      <td>0.645965</td>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.681155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.603600</td>\n",
              "      <td>0.647184</td>\n",
              "      <td>0.633166</td>\n",
              "      <td>0.682168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.594800</td>\n",
              "      <td>0.647356</td>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.681459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.596300</td>\n",
              "      <td>0.645033</td>\n",
              "      <td>0.633166</td>\n",
              "      <td>0.682371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.603400</td>\n",
              "      <td>0.645196</td>\n",
              "      <td>0.628141</td>\n",
              "      <td>0.682371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.607900</td>\n",
              "      <td>0.644870</td>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.682573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.601600</td>\n",
              "      <td>0.644508</td>\n",
              "      <td>0.633166</td>\n",
              "      <td>0.682675</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='650' max='650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [650/650 48:33, Epoch 50/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Roc Auc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.678600</td>\n",
              "      <td>0.681775</td>\n",
              "      <td>0.547739</td>\n",
              "      <td>0.628571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.658700</td>\n",
              "      <td>0.668000</td>\n",
              "      <td>0.613065</td>\n",
              "      <td>0.624417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.644900</td>\n",
              "      <td>0.665685</td>\n",
              "      <td>0.613065</td>\n",
              "      <td>0.628166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.630800</td>\n",
              "      <td>0.663865</td>\n",
              "      <td>0.613065</td>\n",
              "      <td>0.631307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.624800</td>\n",
              "      <td>0.664629</td>\n",
              "      <td>0.613065</td>\n",
              "      <td>0.638804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.620600</td>\n",
              "      <td>0.662817</td>\n",
              "      <td>0.603015</td>\n",
              "      <td>0.644681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.622100</td>\n",
              "      <td>0.670263</td>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.647720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.620300</td>\n",
              "      <td>0.666841</td>\n",
              "      <td>0.608040</td>\n",
              "      <td>0.650051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.624700</td>\n",
              "      <td>0.662564</td>\n",
              "      <td>0.608040</td>\n",
              "      <td>0.650963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.613400</td>\n",
              "      <td>0.659566</td>\n",
              "      <td>0.618090</td>\n",
              "      <td>0.655117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.621200</td>\n",
              "      <td>0.664817</td>\n",
              "      <td>0.608040</td>\n",
              "      <td>0.659068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.616800</td>\n",
              "      <td>0.657235</td>\n",
              "      <td>0.618090</td>\n",
              "      <td>0.661601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.605900</td>\n",
              "      <td>0.656766</td>\n",
              "      <td>0.613065</td>\n",
              "      <td>0.664032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.613000</td>\n",
              "      <td>0.657825</td>\n",
              "      <td>0.618090</td>\n",
              "      <td>0.666363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.615400</td>\n",
              "      <td>0.654676</td>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.668997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.596900</td>\n",
              "      <td>0.653403</td>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.669504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.614800</td>\n",
              "      <td>0.655740</td>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.671935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.606100</td>\n",
              "      <td>0.651890</td>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.672543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.608000</td>\n",
              "      <td>0.650838</td>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.672948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.611500</td>\n",
              "      <td>0.653252</td>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.674063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.601000</td>\n",
              "      <td>0.651536</td>\n",
              "      <td>0.628141</td>\n",
              "      <td>0.674975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.609900</td>\n",
              "      <td>0.651449</td>\n",
              "      <td>0.628141</td>\n",
              "      <td>0.675684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.597200</td>\n",
              "      <td>0.648688</td>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.676089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.599500</td>\n",
              "      <td>0.648820</td>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.676292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.603600</td>\n",
              "      <td>0.649136</td>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.677001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.598300</td>\n",
              "      <td>0.648966</td>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.676190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.608700</td>\n",
              "      <td>0.648253</td>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.677710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.599300</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.678116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.602500</td>\n",
              "      <td>0.646846</td>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.680243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.595400</td>\n",
              "      <td>0.646123</td>\n",
              "      <td>0.633166</td>\n",
              "      <td>0.680142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.599600</td>\n",
              "      <td>0.647905</td>\n",
              "      <td>0.628141</td>\n",
              "      <td>0.680648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.601800</td>\n",
              "      <td>0.646105</td>\n",
              "      <td>0.633166</td>\n",
              "      <td>0.681054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.594000</td>\n",
              "      <td>0.645890</td>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.680750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.599200</td>\n",
              "      <td>0.645965</td>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.681155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.603600</td>\n",
              "      <td>0.647184</td>\n",
              "      <td>0.633166</td>\n",
              "      <td>0.682168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.594800</td>\n",
              "      <td>0.647356</td>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.681459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.596300</td>\n",
              "      <td>0.645033</td>\n",
              "      <td>0.633166</td>\n",
              "      <td>0.682371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.603400</td>\n",
              "      <td>0.645196</td>\n",
              "      <td>0.628141</td>\n",
              "      <td>0.682371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.607900</td>\n",
              "      <td>0.644870</td>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.682573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.601600</td>\n",
              "      <td>0.644508</td>\n",
              "      <td>0.633166</td>\n",
              "      <td>0.682675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.602700</td>\n",
              "      <td>0.644863</td>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.683384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.590900</td>\n",
              "      <td>0.644167</td>\n",
              "      <td>0.638191</td>\n",
              "      <td>0.683080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.604100</td>\n",
              "      <td>0.644565</td>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.683181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.595200</td>\n",
              "      <td>0.643786</td>\n",
              "      <td>0.638191</td>\n",
              "      <td>0.683587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.603100</td>\n",
              "      <td>0.643752</td>\n",
              "      <td>0.633166</td>\n",
              "      <td>0.683384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.602100</td>\n",
              "      <td>0.643734</td>\n",
              "      <td>0.638191</td>\n",
              "      <td>0.683485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.596900</td>\n",
              "      <td>0.643779</td>\n",
              "      <td>0.638191</td>\n",
              "      <td>0.683384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.597700</td>\n",
              "      <td>0.643697</td>\n",
              "      <td>0.638191</td>\n",
              "      <td>0.683384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.599500</td>\n",
              "      <td>0.643693</td>\n",
              "      <td>0.638191</td>\n",
              "      <td>0.683283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.596800</td>\n",
              "      <td>0.643726</td>\n",
              "      <td>0.633166</td>\n",
              "      <td>0.683283</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=650, training_loss=0.6091061012561505, metrics={'train_runtime': 2921.1399, 'train_samples_per_second': 13.556, 'train_steps_per_second': 0.223, 'total_flos': 1.0419197792256e+16, 'train_loss': 0.6091061012561505, 'epoch': 50.0})"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
        "model.save_pretrained('/content/drive/MyDrive/DocsNew1/rubioBERT_finetuned')\n",
        "tokenizer.save_pretrained('/content/drive/MyDrive/DocsNew1/rubioBERT_finetuned_tokenizer')"
      ],
      "metadata": {
        "id": "QKnWbbTq2QcH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9d08c36-81cc-4a50-9e51-9f389467db75"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/DocsNew1/rubioBERT_finetuned_tokenizer/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/DocsNew1/rubioBERT_finetuned_tokenizer/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/DocsNew1/rubioBERT_finetuned_tokenizer/vocab.txt',\n",
              " '/content/drive/MyDrive/DocsNew1/rubioBERT_finetuned_tokenizer/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    }
  ]
}